{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e72c2d",
   "metadata": {},
   "source": [
    "# Analysis of Bias and Variance in a linear regression model\n",
    "In this notebook, using a salary prediction dataset called salary prediction dataset https://www.kaggle.com/datasets/rkiattisak/salaly-prediction-for-beginer \n",
    ",we will analyze the bias and prediction using different features of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c06759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f570be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "\n",
    "salary_data = pd.read_csv(\"data/Salary Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d672f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "salary_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb2384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows that have missing values\n",
    "salary_data = salary_data.dropna()\n",
    "salary_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d3dfba",
   "metadata": {},
   "source": [
    "### We want to build a model that can predict salary based on the following features:\n",
    "- Age\n",
    "- Educational Level\n",
    "- Years of Experience\n",
    "- Gender\n",
    "- Job Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb2351",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Age', 'Gender', 'Education Level', 'Years of Experience', 'Job Title', 'Salary']\n",
    "salary_data = salary_data[features]\n",
    "salary_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe2a1c",
   "metadata": {},
   "source": [
    "### Let's examine the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480af3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(salary_data['Gender'].unique())\n",
    "print(salary_data['Education Level'].unique())\n",
    "print(salary_data['Job Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be19bca",
   "metadata": {},
   "source": [
    "### We will use LabelEncoder to conver Categorical values to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a49137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder (df):\n",
    "    columnsToEncode = list(df.select_dtypes(include = ['category', 'object']))\n",
    "    le = LabelEncoder()\n",
    "    for feature in columnsToEncode:\n",
    "        try:\n",
    "            df[feature] = le.fit_transform(df[feature])\n",
    "        except:\n",
    "            print('Error encoding '+feature)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27cdb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['Gender', 'Education Level', 'Job Title']\n",
    "salary_data = Encoder(salary_data)\n",
    "salary_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5540c5a1",
   "metadata": {},
   "source": [
    "## Regression Model\n",
    "### 1. Let's try with only one feature (input), for example years of experience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f43e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary = salary_data[['Years of Experience', 'Salary']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d49485",
   "metadata": {},
   "source": [
    "### 1.1 Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5117a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x = 'Years of Experience', y='Salary', data = df_binary, order = 2, ci = None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2053a5e",
   "metadata": {},
   "source": [
    "### 1.2 Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb513f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into indpendent (X) and dependent variables \n",
    "# Convert the data frame into a numpy array since each dataframe contains only one column\n",
    "X = np.array(df_binary['Years of Experience']).reshape(-1,1)\n",
    "y = np.array(df_binary['Salary']).reshape(-1,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16be183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split the data\n",
    "def prepare_train_cv_test (X, y):\n",
    "    \n",
    "    # get 60% of the dataset as the training set.  Put the remaining 40% in temporary variables\n",
    "    X_train, X_, y_train, y_ = train_test_split(X, y, test_size = 0.40, random_state = 55)\n",
    "\n",
    "    # Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "    X_cv, X_test, y_cv, y_test = train_test_split(X_, y_,  test_size = .50, random_state = 55)\n",
    "    \n",
    "    return X_train, y_train, X_cv, y_cv, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9810e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_cv, y_cv, X_test, y_test = prepare_train_cv_test(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f39361",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_cv = model.predict(X_cv)\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", model.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_cv, y_pred_cv))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_cv, y_pred_cv))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(X_cv, y_cv, color=\"black\")\n",
    "plt.plot(X_cv, y_pred_cv, color=\"blue\", linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "cv_mse = mean_squared_error(y_cv, y_pred_cv) \n",
    "print(f\"Training MSE : {train_mse:.0f}\")\n",
    "print(f\"Cross Validation MSE: {cv_mse: .0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fee582",
   "metadata": {},
   "source": [
    "#### The results show that the both training and Cross Validation Errors are high. This maybe due to high bias (underfit). We could one or more of the following:\n",
    "\n",
    "- Try adding additional features\n",
    "- Try decreasing the regulatiztion parameter\n",
    "- Try adding polynomial features\n",
    "\n",
    "We will try the first option\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d1c5d",
   "metadata": {},
   "source": [
    "### 2.1 Let's try adding more features, age, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9812083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = salary_data[['Age', 'Years of Experience', 'Gender', 'Job Title']]\n",
    "y = salary_data['Salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2bde0c",
   "metadata": {},
   "source": [
    "### We will now split the data into train, cross validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split the data\n",
    "def prepare_train_cv_test (X, y):\n",
    "    \n",
    "    # get 60% of the dataset as the training set.  Put the remaining 40% in temporary variables\n",
    "    X_train, X_, y_train, y_ = train_test_split(X, y, test_size = 0.40, random_state = 55)\n",
    "\n",
    "    # Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "    X_cv, X_test, y_cv, y_test = train_test_split(X_, y_,  test_size = .50, random_state = 55)\n",
    "    \n",
    "    return X_train, y_train, X_cv, y_cv, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_cv, y_cv, X_test, y_test = prepare_train_cv_test (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2a9e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Training Data shape: {X_train.shape}\")\n",
    "print(f\" Cross Validation Data shape: {X_cv.shape}\")\n",
    "print(f\" Test Data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c97538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data has different range of values, for example age and years of experience have a large range than \n",
    "# Gender and Educational Level.  We want to scale the data into new values that are easier to compare\n",
    "scale = StandardScaler()\n",
    "X_train_scaled = scale.fit_transform(X_train)\n",
    "X_cv_scaled = scale.fit_transform(X_cv)\n",
    "X_test_scaled = scale.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b89f17",
   "metadata": {},
   "source": [
    "###  2.2 Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00a6c3",
   "metadata": {},
   "source": [
    "### 2.3 Explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616148c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cv = model.predict(X_cv_scaled)\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", model.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_cv, y_pred_cv))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_cv, y_pred_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8caf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "cv_mse = mean_squared_error(y_cv, y_pred_cv)\n",
    "print(f\"Training MSE : {train_mse:.0f}\")\n",
    "print(f\"Cross Validation MSE: {cv_mse: .0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573e5aae",
   "metadata": {},
   "source": [
    "The accuracy the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79257a68",
   "metadata": {},
   "source": [
    "The model is performing better.  Both training and Cross Validation MSE were decreased.  Can we improve it furthere?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079bdfb",
   "metadata": {},
   "source": [
    "### 3.1 Let's try addition polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de7bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_train_mapped = poly.fit_transform(X_train)\n",
    "X_cv_mapped = poly.fit_transform(X_cv)\n",
    "X_test_mapped = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa7559",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mapped_scaled = scale.fit_transform(X_train_mapped)\n",
    "X_cv_mapped_scaled = scale.fit_transform(X_cv_mapped)\n",
    "X_test_mapped_scaled = scale.fit_transform(X_test_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0b666",
   "metadata": {},
   "source": [
    "###  3.2 Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_mapped_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81290139",
   "metadata": {},
   "source": [
    "### 3.3 Explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f0dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cv = model.predict(X_cv_mapped_scaled)\n",
    "y_pred_train = model.predict(X_train_mapped_scaled)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", model.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_cv, y_pred_cv))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_cv, y_pred_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2b119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cv = model.predict(X_cv_mapped_scaled)\n",
    "y_pred_train = model.predict(X_train_mapped_scaled)\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "cv_mse = mean_squared_error(y_cv, y_pred_cv)\n",
    "print(f\"Training MSE : {train_mse:.0f}\")\n",
    "print(f\"Cross Validation MSE: {cv_mse: .0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c0d9e4",
   "metadata": {},
   "source": [
    "The model is performing even better.  Both training and Cross Validation MSE were decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41044c65",
   "metadata": {},
   "source": [
    "### 4.1 Let's try various degrees of polynomial to see if our model gets more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7790c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=None):\n",
    "    \n",
    "    train_mses = []\n",
    "    cv_mses = []\n",
    "    models = []\n",
    "    scalers = []\n",
    "    degrees = range(1,max_degree+1)\n",
    "\n",
    "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "    for degree in degrees:\n",
    "        print(f\"Dgree {degree}:\")\n",
    "        # Add polynomial features to the training set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "        # Scale the training set\n",
    "        scaler_poly = StandardScaler()\n",
    "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "        scalers.append(scaler_poly)\n",
    "\n",
    "        # Create and train the model\n",
    "        model.fit(X_train_mapped_scaled, y_train )\n",
    "        models.append(model)\n",
    "\n",
    "        # Compute the training MSE\n",
    "        yhat = model.predict(X_train_mapped_scaled)\n",
    "        train_mse = mean_squared_error(y_train, yhat)\n",
    "        print(f\"     Train mse: {train_mse: .0f}\")\n",
    "        train_mses.append(train_mse)\n",
    "\n",
    "        # Add polynomial features and scale the cross-validation set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_cv_mapped = poly.fit_transform(x_cv)\n",
    "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "        # Compute the cross-validation MSE\n",
    "        yhat = model.predict(X_cv_mapped_scaled)\n",
    "        cv_mse = mean_squared_error(y_cv, yhat)\n",
    "        print(f\"     cv mse: {cv_mse: .0f}\")\n",
    "        cv_mses.append(cv_mse)\n",
    "        print(\"      Coefficient of determination (r2 score): %.2f\" % r2_score(y_cv, yhat))\n",
    "\n",
    "    # Plot the results\n",
    "    \n",
    "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs'); \n",
    "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs'); \n",
    "    plt.plot(degrees, np.repeat(baseline, len(degrees)), linestyle='--', label='baseline')\n",
    "    plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
    "    plt.xticks(degrees)\n",
    "    plt.xlabel(\"degree\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fa43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_cv, y_cv, X_test, y_test = prepare_train_cv_test (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Training Data shape: {X_train.shape}\")\n",
    "print(f\" Cross Validation Data shape: {X_cv.shape}\")\n",
    "print(f\" Test Data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f2f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot_poly(model, X_train, y_train, X_cv, y_cv, max_degree =10, baseline= 122122122)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc433a",
   "metadata": {},
   "source": [
    "Based on the above results polynomial of degree 3 seems to be best fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData2",
   "language": "python",
   "name": "pythondata2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
